{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os, time\n",
    "import glob\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# these magics ensure that external modules that are modified are also automatically reloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# widgets and interaction\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gzip\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../models/dcgan/\")\n",
    "\n",
    "import models.dcgan as dcgan\n",
    "import models.mlp as mlp\n",
    "import models.dcgan_orig as do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_list(s):\n",
    "    s = re.sub('\\s+', ' ', s)[2:-2]\n",
    "    return tuple([float(n.strip()) for n in s.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = pd.read_csv(\"/home/data/world-cities/urban_areas_over_10kpop_stats.csv\")\n",
    "files_df['patch distr 286'] = files_df['patch distr 286'].apply(parse_list)\n",
    "files_df['patch distr 128'] = files_df['patch distr 128'].apply(parse_list)\n",
    "files_df['patch distr 64'] = files_df['patch distr 64'].apply(parse_list)\n",
    "\n",
    "files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(range(len(files_df)), size=int(0.8*len(files_df)), replace=False)\n",
    "train_df = files_df.iloc[idx]\n",
    "valid_df = files_df.iloc[list(set(range(len(files_df)))-set(idx))]\n",
    "\n",
    "print len(train_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(\"/home/data/world-cities/\" + \"train.csv\")\n",
    "valid_df.to_csv(\"/home/data/world-cities/\" + \"valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pxCrop    = 286\n",
    "classCol  = \"build pct 128\"\n",
    "imageSize = 128\n",
    "batchSize = 64\n",
    "workers   = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../pytorch_utils\")\n",
    "from loader_dataframe import ImageDataFrame, grayscale_loader\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def fn_rotate(img, max_angle=30):\n",
    "    theta = np.random.randn()*max_angle\n",
    "    return img.rotate(theta, expand=True)\n",
    "\n",
    "train_dataset = ImageDataFrame(df=train_df, \n",
    "                         label_columns=[classCol],\n",
    "                         loader=grayscale_loader,\n",
    "                         transform=transforms.Compose([\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.CenterCrop(pxCrop),\n",
    "                               transforms.Lambda(lambda img: fn_rotate(img)),\n",
    "                               transforms.Scale(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0, 0, 0), (255.0, 255.0, 255.0))\n",
    "                           ]))\n",
    "valid_dataset = ImageDataFrame(df=valid_df, \n",
    "                         label_columns=[classCol],\n",
    "                         loader=grayscale_loader,\n",
    "                         transform=transforms.Compose([\n",
    "                               transforms.CenterCrop(pxCrop),\n",
    "                               transforms.Scale(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0, 0, 0), (255.0, 255.0, 255.0))\n",
    "\n",
    "                           ]))\n",
    "\n",
    "list_output = train_df[classCol].dtype == object\n",
    "if list_output:\n",
    "    n_classes = len(train_df[classCol].iloc[0])\n",
    "else:\n",
    "    n_classes = len(train_dataset.classes) if train_dataset.classes is not None else None\n",
    "\n",
    "weights = None #train_df['built pct']\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batchSize,\n",
    "                                         shuffle=True, \n",
    "                                         #sampler=WeightedRandomSampler(weights, len(train_df)),\n",
    "                                         num_workers=int(workers))\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batchSize,\n",
    "                                         shuffle=False, \n",
    "                                         num_workers=int(workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model\n",
    "- should work on arbitrary size images (multiple of 16)\n",
    "- dual output: 0/1 for fake/real, vector of stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100\n",
    "nc = 1\n",
    "ndf= 16\n",
    "ngpu = 4\n",
    "lr = 0.00005\n",
    "n_extra_layers = 0\n",
    "beta1 = 0.5\n",
    "n_classes = 1\n",
    "lam = 0.5 # torch.FloatTensor(n_classes).fill_(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class DCGAN_D_DUAL(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ndf, ngpu, n_extra_layers=0, n_classes=None):\n",
    "        super(DCGAN_D_DUAL, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        features = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        features.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        features.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            features.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            features.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            features.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            features.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            features.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            features.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        features.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(cndf, cndf, 4, 1, 0, bias=False))\n",
    "        self.features = features\n",
    "        self.flat_fts = self.get_flat_fts((nc,isize,isize), self.features)\n",
    "\n",
    "        self.classifier_src = nn.Sequential(\n",
    "            nn.Linear(self.flat_fts, 1),\n",
    "#             nn.Dropout(p=0.2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(100,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        if n_classes is None:\n",
    "            n_classes = 1\n",
    "        self.classifier_cls = nn.Sequential(\n",
    "            nn.Linear(self.flat_fts, n_classes),\n",
    "#             nn.Dropout(p=0.2),\n",
    "             nn.ReLU(),\n",
    "#             nn.Linear(100,n_classes),\n",
    "#             nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def get_flat_fts(self, in_size, fts):\n",
    "        f = fts(Variable(torch.ones(1,*in_size)))\n",
    "        ret = int(np.prod(f.size()[1:]))\n",
    "        print ret\n",
    "        return ret\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fts = self.features(x)\n",
    "        flat_fts = fts.view(-1, self.flat_fts)\n",
    "        out1 = self.classifier1(flat_fts)\n",
    "        out2 = self.classifier2(flat_fts)\n",
    "        return out1, out2\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            fts = nn.parallel.data_parallel(self.features, input, range(self.ngpu))\n",
    "            flat_fts = fts.view(-1, self.flat_fts)\n",
    "            out_src= nn.parallel.data_parallel(self.classifier_src, flat_fts, range(self.ngpu))\n",
    "            out_cls= nn.parallel.data_parallel(self.classifier_cls, flat_fts, range(self.ngpu))            \n",
    "        else: \n",
    "            fts = self.features(input)\n",
    "            flat_fts = fts.view(-1, self.flat_fts)\n",
    "            out_src= self.classifier_src(flat_fts)\n",
    "            out_cls= self.classifier_cls(flat_fts)\n",
    "                \n",
    "        return out_src, out_cls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DCGAN_D_DUAL(imageSize, nz, nc, ndf, ngpu, n_extra_layers, n_classes=n_classes)\n",
    "net.apply(weights_init)\n",
    "    \n",
    "input = torch.FloatTensor(batchSize, nc, imageSize, imageSize)\n",
    "if n_classes is not None:\n",
    "    if not list_output:\n",
    "        label = torch.FloatTensor(batchSize, 1)\n",
    "    else:\n",
    "        label = torch.FloatTensor(batchSize, n_classes)\n",
    "else:\n",
    "    label = torch.FloatTensor(batchSize, 1)\n",
    "    \n",
    "source_label = torch.FloatTensor(batchSize)\n",
    "\n",
    "# lam = Variable(lam)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "    input = input.cuda()\n",
    "    label, source_label = label.cuda(), source_label.cuda()\n",
    "    # lam = lam.cuda()\n",
    "\n",
    "input = Variable(input)\n",
    "label = Variable(label, requires_grad=False)\n",
    "source_label = Variable(source_label, requires_grad=False)\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# optimization criteria\n",
    "criterion_bce = nn.BCELoss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "criterion_cls = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def compute_eval_performance(net, valid_dataloader):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for imgs, labs in valid_dataloader:\n",
    "#         imgs = Variable(imgs.cuda())\n",
    "#         labs = labs.cuda()\n",
    "#         out_src_test, out_sts_test = net(imgs)\n",
    "#         loss_src_test = criterion_bce(out_src_test, source_label)\n",
    "#         loss_sts_test = criterion_cls(out_sts_test, labs)\n",
    "#         _, predicted = torch.max(out_sts_test.data, 1)\n",
    "#         total += labs.size(0)\n",
    "#         correct += (predicted == labs).sum()\n",
    "    \n",
    "#     val_acc = correct / float(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "save_dir = \"/home/data/pytorch-workspace/analyzer/classifier/\"\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "train_loss_hist = []\n",
    "valid_loss_hist = []\n",
    "best_val_perf  = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Train for this epoch\n",
    "    # --------------------\n",
    "    \n",
    "    net.train(True)\n",
    "    train_loss_epoch = []\n",
    "    for i, (imgs, labs) in enumerate(train_dataloader):    \n",
    "        labs = torch.cat(labs, len(labs)>1)\n",
    "        batch_size = imgs.size(0)\n",
    "        input.data.resize_(imgs.size()).copy_(imgs)\n",
    "        label.data.resize_(labs.size()).copy_(labs)\n",
    "        source_label.data.resize_(batch_size).fill_(1) # right now, all images are real\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        net.zero_grad()\n",
    "        out_src, out_sts = net(input)\n",
    "        loss_src = criterion_bce(out_src, source_label)\n",
    "        loss_sts = criterion_mse(out_sts * lam, label * lam) # lam can be a vector of weights\n",
    "        # loss = 0*loss_src + lam * loss_sts    \n",
    "        loss = loss_sts\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f (Source: %.4f, Stats: %.4f)' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, \n",
    "                     loss.data[0], loss_src.data[0], loss_sts.data[0]*lam))\n",
    "        train_loss_epoch.append((loss.data[0], loss_src.data[0], loss_sts.data[0]*lam))\n",
    "    \n",
    "    train_loss_epoch = np.array(train_loss_epoch).mean(0)\n",
    "    train_loss_hist.append(train_loss_epoch)\n",
    "    \n",
    "    # Test for this epoch\n",
    "    # -------------------\n",
    "\n",
    "    net.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    valid_loss_epoch = []\n",
    "    for imgs, labs in valid_dataloader:\n",
    "        labs = torch.cat(labs, len(labs)>1\n",
    "                        \n",
    "                        \n",
    "                        )\n",
    "        imgs = Variable(imgs.cuda())\n",
    "        labs = Variable(labs.float().cuda())\n",
    "        src_labs = Variable(torch.FloatTensor(labs.data.size(0)).cuda(), requires_grad=False)\n",
    "        out_src_valid, out_sts_valid = net(imgs)\n",
    "        loss_src_valid = criterion_bce(out_src_valid, src_labs)\n",
    "        loss_sts_valid = criterion_mse(out_sts_valid * lam, labs * lam)\n",
    "        loss_valid = loss_sts # + loss_src\n",
    "        valid_loss_epoch.append((loss_valid.data[0], loss_src_valid.data[0], loss_sts_valid.data[0]))   \n",
    "        \n",
    "    valid_loss_epoch = np.array(valid_loss_epoch).mean(0)\n",
    "    valid_loss_hist.append(valid_loss_epoch)\n",
    "    \n",
    "    # Track performance\n",
    "    # -----------------\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # plot performance vs epoch\n",
    "    fig, ax = plt.subplots(1,2, figsize=(8,3))\n",
    "    ax[0].plot(range(len(train_loss_hist)), \n",
    "             [x[0] for x in train_loss_hist],label=\"loss (train)\")\n",
    "    ax[0].plot(range(len(train_loss_hist)), \n",
    "             [x[2] for x in train_loss_hist],label=\"stat loss (train)\")\n",
    "    ax[0].legend(loc=\"best\")\n",
    "    ax[1].plot(range(len(valid_loss_hist)), \n",
    "             [x[0] for x in valid_loss_hist],label=\"loss (valid)\")\n",
    "    ax[1].plot(range(len(valid_loss_hist)), \n",
    "             [x[2] for x in valid_loss_hist],label=\"stat loss (valid)\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    plt.suptitle(\"Analyzer model performance\")\n",
    "    plt.savefig(\"%s/training_progress.jpg\"%save_dir)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "#     # checkpoint best model\n",
    "#     if val_acc > best_val_acc:\n",
    "#         print \"checkpointing: validation accuracy improved from %.2f to %.2f\"%(best_val_acc, val_acc)\n",
    "#         torch.save(net.state_dict(), '{0}/net_epoch{1}_acc{2:.2f}.pth'.format(save_dir, epoch, val_acc))\n",
    "#         best_val_acc = val_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sq(((out_sts - label)**2)).dot(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(imread(\"%s/training_progress.jpg\"%save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38dcc31db03b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
