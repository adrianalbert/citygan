{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial regression analysis with GANs: Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents analysis on using GANs as a tool for spatial regression. Here, we focus on the Generator model $G$ and investigate several questions:\n",
    "\n",
    "* can we use a trained Generator model $G$ to estimate gradients via backpropagation?\n",
    "* can we estimate the spatial dependence of the gradient, i.e., the amount of \"change\" that stays in an originating region vs the \"spillover\" of that change outside the originating region?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os, time\n",
    "import glob\n",
    "import timeit\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# these magics ensure that external modules that are modified are also automatically reloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# widgets and interaction\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, grad\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "sys.path.append(\"../models/pytorch-CycleGAN-and-pix2pix/\")\n",
    "\n",
    "from models.models import create_model\n",
    "from data.data_loader import CreateDataLoader\n",
    "from util.visualizer import Visualizer\n",
    "from pdb import set_trace as st\n",
    "from util import html\n",
    "\n",
    "sys.path.append(\"../pytorch_utils\")\n",
    "from loader_dataframe import ImageDataFrame, fn_rotate, grayscale_loader, default_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse options file to load model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_value(v):\n",
    "    try:\n",
    "        v = float(v)\n",
    "        if np.isinf(v):\n",
    "            return v\n",
    "        if np.round(v,0)==v:\n",
    "            v = int(v)\n",
    "        return v\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if v == 'False':\n",
    "        return False\n",
    "    elif v == 'True':\n",
    "        return True\n",
    "    elif '|' in v:\n",
    "        v = v.split(\"|\")\n",
    "    elif v == '[]':\n",
    "        v = []\n",
    "    elif '[' in v:\n",
    "        v = [int(x) if \"'\" not in x and '\"' not in x else x.replace(\"'\",\"\").replace('\"',\"\") \\\n",
    "             for x in v[1:-1].split(\", \")]\n",
    "    else:\n",
    "        pass\n",
    "    return v\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "def parse_opt_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [tuple(l.replace(\"\\n\",\"\").split(': ')) for l in lines[1:-1]] \n",
    "    lines = [(x[0], parse_value(x[1])) for x in lines]\n",
    "    ret = namedtuple(\"opt\", [l[0].strip() for l in lines])\n",
    "    for l in lines:\n",
    "        setattr(ret, l[0], l[1])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoints_path = \"/home/data/pytorch-workspace/urban-form-checkpoints/\"\n",
    "\n",
    "experiment_path = \"pix2pix_reg_bceloss_af_nowater\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_filename = checkpoints_path + \"checkpoints/\" + experiment_path + \"/opt.txt\"\n",
    "\n",
    "opt = parse_opt_file(opt_filename)\n",
    "opt.checkpoints_dir = checkpoints_path + opt.checkpoints_dir\n",
    "opt.batchSize = 1\n",
    "if opt.label_columns == \"\":\n",
    "    opt.label_columns = []\n",
    "if hasattr(opt, 'cond_columns') and opt.cond_columns == \"\":\n",
    "    opt.cond_columns = []\n",
    "    \n",
    "opt.nThreads = 1   # test code only supports nThreads=1\n",
    "opt.batchSize = 1  #test code only supports batchSize=1\n",
    "opt.serial_batches = True # no shuffle\n",
    "# opt.gpu_ids = '-1'\n",
    "opt.noise_amt = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models G and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pix2pix-reg\n",
      "---------- Networks initialized -------------\n",
      "UnetGenerator (\n",
      "  (model): UnetSkipConnectionBlock (\n",
      "    (model): Sequential (\n",
      "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): UnetSkipConnectionBlock (\n",
      "        (model): Sequential (\n",
      "          (0): LeakyReLU (0.2, inplace)\n",
      "          (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (3): UnetSkipConnectionBlock (\n",
      "            (model): Sequential (\n",
      "              (0): LeakyReLU (0.2, inplace)\n",
      "              (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "              (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "              (3): UnetSkipConnectionBlock (\n",
      "                (model): Sequential (\n",
      "                  (0): LeakyReLU (0.2, inplace)\n",
      "                  (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                  (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                  (3): UnetSkipConnectionBlock (\n",
      "                    (model): Sequential (\n",
      "                      (0): LeakyReLU (0.2, inplace)\n",
      "                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                      (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                      (3): UnetSkipConnectionBlock (\n",
      "                        (model): Sequential (\n",
      "                          (0): LeakyReLU (0.2, inplace)\n",
      "                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                          (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                          (3): UnetSkipConnectionBlock (\n",
      "                            (model): Sequential (\n",
      "                              (0): LeakyReLU (0.2, inplace)\n",
      "                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                              (2): ReLU (inplace)\n",
      "                              (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                              (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                            )\n",
      "                          )\n",
      "                          (4): ReLU (inplace)\n",
      "                          (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                          (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                          (7): Dropout (p = 0.5)\n",
      "                        )\n",
      "                      )\n",
      "                      (4): ReLU (inplace)\n",
      "                      (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                      (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "                      (7): Dropout (p = 0.5)\n",
      "                    )\n",
      "                  )\n",
      "                  (4): ReLU (inplace)\n",
      "                  (5): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "                  (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "                )\n",
      "              )\n",
      "              (4): ReLU (inplace)\n",
      "              (5): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "              (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "            )\n",
      "          )\n",
      "          (4): ReLU (inplace)\n",
      "          (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "          (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ReLU (inplace)\n",
      "      (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (4): Tanh ()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 41833475\n",
      "NLayerDiscriminatorReg (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU (0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU (0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU (0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): LeakyReLU (0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (classifier_src): Sequential (\n",
      "    (0): Linear (900 -> 1)\n",
      "    (source classifier): Sigmoid ()\n",
      "  )\n",
      "  (classifier_cls): Sequential (\n",
      "    (0): Linear (900 -> 50)\n",
      "    (1): Dropout (p = 0.2)\n",
      "    (2): LeakyReLU (0.2, inplace)\n",
      "    (3): Linear (50 -> 50)\n",
      "    (4): Dropout (p = 0.2)\n",
      "    (5): LeakyReLU (0.2, inplace)\n",
      "    (6): Linear (50 -> 2)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 2818204\n",
      "-----------------------------------------------\n",
      "model [Pix2PixModel] was created\n"
     ]
    }
   ],
   "source": [
    "opt.phase = \"test\"\n",
    "opt.which_epoch = '85'\n",
    "\n",
    "model = create_model(opt)\n",
    "\n",
    "model.load_network(model.netG, 'G', opt.which_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename_SAR</th>\n",
       "      <th>class</th>\n",
       "      <th>country</th>\n",
       "      <th>population</th>\n",
       "      <th>city</th>\n",
       "      <th>region</th>\n",
       "      <th>source</th>\n",
       "      <th>build pct 128</th>\n",
       "      <th>patch distr 128</th>\n",
       "      <th>top patch areas 128</th>\n",
       "      <th>...</th>\n",
       "      <th>box counts 286</th>\n",
       "      <th>build pct 64</th>\n",
       "      <th>patch distr 64</th>\n",
       "      <th>top patch areas 64</th>\n",
       "      <th>fractal dim 64</th>\n",
       "      <th>box counts 64</th>\n",
       "      <th>decile</th>\n",
       "      <th>basename</th>\n",
       "      <th>index</th>\n",
       "      <th>filename_PNM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/data/world-cities/urban_areas_over_10kpo...</td>\n",
       "      <td>very-small</td>\n",
       "      <td>us</td>\n",
       "      <td>16525</td>\n",
       "      <td>swansea, us (pop 16.5k)</td>\n",
       "      <td>Americas</td>\n",
       "      <td>SAR</td>\n",
       "      <td>0.064736</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[20.0, 9.0, 6.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.38629436, 2.19722458, 3.17805383, 4.2195077...</td>\n",
       "      <td>0.061183</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[9.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...</td>\n",
       "      <td>1.396578</td>\n",
       "      <td>[0.0, 1.38629436, 2.07944154, 2.99573227, 3.43...</td>\n",
       "      <td>7</td>\n",
       "      <td>us_swansea,-us-(pop-16.5k)_very-small_16525_41...</td>\n",
       "      <td>12198</td>\n",
       "      <td>/home/data/world-cities/urban_areas_over_10kpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/data/world-cities/urban_areas_over_10kpo...</td>\n",
       "      <td>very-small</td>\n",
       "      <td>de</td>\n",
       "      <td>12299</td>\n",
       "      <td>niefern oschelbronn, de (pop 12.3k)</td>\n",
       "      <td>Europe</td>\n",
       "      <td>SAR</td>\n",
       "      <td>0.102244</td>\n",
       "      <td>[0.69314718, 0.0, 0.69314718, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>[8.0, 5.0, 5.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.38629436, 2.19722458, 3.21887582, 4.3944491...</td>\n",
       "      <td>0.100213</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>1.696283</td>\n",
       "      <td>[0.0, 1.38629436, 2.56494936, 3.52636052, 4.02...</td>\n",
       "      <td>9</td>\n",
       "      <td>de_niefern-oschelbronn,-de-(pop-12.3k)_very-sm...</td>\n",
       "      <td>11861</td>\n",
       "      <td>/home/data/world-cities/urban_areas_over_10kpo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        filename_SAR       class country  \\\n",
       "0  /home/data/world-cities/urban_areas_over_10kpo...  very-small      us   \n",
       "1  /home/data/world-cities/urban_areas_over_10kpo...  very-small      de   \n",
       "\n",
       "   population                                 city    region source  \\\n",
       "0       16525              swansea, us (pop 16.5k)  Americas    SAR   \n",
       "1       12299  niefern oschelbronn, de (pop 12.3k)    Europe    SAR   \n",
       "\n",
       "   build pct 128                                    patch distr 128  \\\n",
       "0       0.064736  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1       0.102244  [0.69314718, 0.0, 0.69314718, 0.0, 0.0, 0.0, 0...   \n",
       "\n",
       "                                 top patch areas 128  \\\n",
       "0  [20.0, 9.0, 6.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0,...   \n",
       "1  [8.0, 5.0, 5.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, ...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "\n",
       "                                      box counts 286 build pct 64  \\\n",
       "0  [1.38629436, 2.19722458, 3.17805383, 4.2195077...     0.061183   \n",
       "1  [1.38629436, 2.19722458, 3.21887582, 4.3944491...     0.100213   \n",
       "\n",
       "                                      patch distr 64  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                  top patch areas 64 fractal dim 64  \\\n",
       "0  [9.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...       1.396578   \n",
       "1  [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...       1.696283   \n",
       "\n",
       "                                       box counts 64 decile  \\\n",
       "0  [0.0, 1.38629436, 2.07944154, 2.99573227, 3.43...      7   \n",
       "1  [0.0, 1.38629436, 2.56494936, 3.52636052, 4.02...      9   \n",
       "\n",
       "                                            basename  index  \\\n",
       "0  us_swansea,-us-(pop-16.5k)_very-small_16525_41...  12198   \n",
       "1  de_niefern-oschelbronn,-de-(pop-12.3k)_very-sm...  11861   \n",
       "\n",
       "                                        filename_PNM  \n",
       "0  /home/data/world-cities/urban_areas_over_10kpo...  \n",
       "1  /home/data/world-cities/urban_areas_over_10kpo...  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(opt.dataroot + \"train.csv\").drop(\"Unnamed: 0\", 1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_val = pd.concat([df[(df['city'].str.contains('barcelona')) & (df['country']=='es') & (df['population']>500000)],\n",
    "                    df[(df['city'].str.contains('san francisco')) & (df['country']=='us') & (df['population']>500000)],\n",
    "                    df[(df['city'].str.contains('paris')) & (df['country']=='fr') & (df['population']>500000)],\n",
    "                    df[(df['city'].str.contains('new delhi')) & (df['country']=='in') & (df['population']>500000)],\n",
    "                    df[(df['city'].str.contains('boston')) & (df['country']=='us') & (df['population']>500000)]])\n",
    "df_val.to_csv(opt.dataroot + \"valid_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv(opt.dataroot + \"valid.csv\").drop(\"Unnamed: 0\", 1)\n",
    "df_valid.rename(columns={\"filename\":\"filename_SAR\"}, inplace=True)\n",
    "df_valid['filename_PNM'] = df_valid['filename_SAR'].apply(lambda x: x.replace(\"SAR\", \"PNM\"))\n",
    "df_valid.to_csv(opt.dataroot + \"valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2996, 23)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignedDataLoaderCSV\n",
      "loading from file /home/data/world-cities/top10k-log//all_sources.csv\n",
      "AlignedDataLoaderCSV\n",
      "loading from file /home/data/world-cities/top10k-log//valid_sample.csv\n",
      "AlignedDataLoaderCSV\n",
      "loading from file /home/data/world-cities/top10k-log//valid.csv\n",
      "AlignedDataLoaderCSV\n",
      "loading from file /home/data/world-cities/top10k-log//train.csv\n",
      "AlignedDataLoaderCSV\n",
      "loading from file /home/data/world-cities/top10k-log//test.csv\n",
      "create web directory /home/data/pytorch-workspace/urban-form-checkpoints/./checkpoints/pix2pix_reg_bceloss_af_nowater/web...\n"
     ]
    }
   ],
   "source": [
    "opt.phase = 'all_sources'\n",
    "data_loader = CreateDataLoader(opt)\n",
    "dataset = data_loader.load_data()\n",
    "\n",
    "opt.phase = 'valid_sample'\n",
    "data_loader_smpl = CreateDataLoader(opt)\n",
    "dataset_smpl = data_loader_smpl.load_data()\n",
    "\n",
    "opt.phase = 'valid'\n",
    "data_loader_valid = CreateDataLoader(opt)\n",
    "dataset_valid = data_loader_valid.load_data()\n",
    "\n",
    "opt.phase = 'train'\n",
    "data_loader_train = CreateDataLoader(opt)\n",
    "dataset_train = data_loader_train.load_data()\n",
    "\n",
    "opt.phase = \"test\"\n",
    "data_loader_test = CreateDataLoader(opt)\n",
    "dataset_test = data_loader_test.load_data()\n",
    "\n",
    "visualizer = Visualizer(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/data/world-cities/top10k-log/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.dataroot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare real and synthetic spatial statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_df = []\n",
    "\n",
    "for i, data in enumerate(dataset_train):\n",
    "    clear_output(wait=True)\n",
    "    print \"%d\"%(i)\n",
    "    \n",
    "    if i == 1000: break\n",
    "    \n",
    "    # perform forward pass using current data\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    \n",
    "    # extract model results\n",
    "    imgA_path, imgB_path = model.get_image_paths()\n",
    "    visuals = model.get_current_visuals()\n",
    "    imgB_np = copy.copy(visuals['real_B'])\n",
    "    imgB_np_fake = copy.copy(visuals['fake_B'])\n",
    "    \n",
    "    # compute labels of generated images\n",
    "    labels_fake_true = compute_stats(imgB_np_fake[:,:,0], imgSize=256)\n",
    "    labels_comp_true = compute_stats(imgB_np[:,:,0], imgSize=256)\n",
    "    \n",
    "    # get labels computed by Analyzer\n",
    "    labels = model.get_current_labels()\n",
    "    labels_fake_pred = labels['labels_fake'][0]\n",
    "    labels_real_true = labels['labels']\n",
    "    \n",
    "    # add stats to list\n",
    "    stats_df.append([imgA_path, imgB_path] + \\\n",
    "                    np.hstack([labels_fake_true, labels_fake_pred, labels_comp_true]).tolist())\n",
    "                 \n",
    "stats_df = pd.DataFrame(stats_df, \n",
    "                        columns=['filenameA', 'filenameB'] + ['a fake', 'f fake'] + \n",
    "                                [x+\" pred\" for x in opt.label_columns] + ['a real', 'f real'])\n",
    "\n",
    "# # plot \n",
    "\n",
    "# sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "# ax = sns.regplot(x=stats_df['a real'], y=stats_df['build pct 286 pred'], x_bins=100)\n",
    "# ax.set_xlabel(\"$a$\")\n",
    "# ax.set_ylabel(\"$\\hat a$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "plt.figure(figsize=(4,4))\n",
    "ax = sns.regplot(x=stats_df['a real'], y= stats_df['a fake'], x_bins=100)\n",
    "ax.set_xlabel(\"$a$\")\n",
    "ax.set_ylabel(\"$\\hat a$\")\n",
    "ax.set_title(\"Built area: \\n synthetic ($\\hat a$) vs true ($a$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.regplot(x=stats_df['f real'], y= stats_df['f fake'], x_bins=100)\n",
    "ax.set_xlabel(\"$f$\")\n",
    "ax.set_ylabel(\"$\\hat f$\")\n",
    "ax.set_title(\"Fractal dimension: \\n synthetic ($\\hat f$) vs true ($f$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sns.set_context(\"notebook\", font_scale=1.3)\n",
    "\n",
    "# g = sns.jointplot(stats_df['a real'], stats_df['f real'], \n",
    "#                   ylim=(1.8,2.0),\n",
    "#                   kind=\"hex\", color=\"#4CB391\")\n",
    "# g.fig.suptitle(\"Built area $a$ vs fractal dim. $f$ (real)\", fontsize=16)\n",
    "# g.ax_joint.set_xlabel(\"$a$\")\n",
    "# g.ax_joint.set_ylabel(\"$f$\")\n",
    "\n",
    "# sns.set_context(\"notebook\", font_scale=1.3)\n",
    "\n",
    "# g = sns.jointplot(stats_df['a fake'], stats_df['f fake'], \n",
    "#                   ylim=(1.8,2.0),\n",
    "#                   kind=\"hex\", color=\"#4CB391\")\n",
    "# g.fig.suptitle(\"Built area $\\hat a$ vs fractal dim. $\\hat f$ (pred.)\", fontsize=16)\n",
    "# g.ax_joint.set_xlabel(\"$\\hat a$\", fontsize=16)\n",
    "# g.ax_joint.set_ylabel(\"$\\hat f$\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate distance dependence of population density and luminosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820 / 2996: 4.44 sec\n"
     ]
    }
   ],
   "source": [
    "which_dataset = dataset_valid\n",
    "info_filename = data_loader_valid.filename_csv\n",
    "info_df = pd.read_csv(info_filename)\n",
    "\n",
    "dist_pop_mu = []\n",
    "dist_lum_mu = []\n",
    "dist_pop_se = []\n",
    "dist_lum_se = []\n",
    "patch_stats = []\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for i, data in enumerate(which_dataset):\n",
    "\n",
    "    AtoB = model.opt.which_direction == 'AtoB'\n",
    "    imgA = data['A' if AtoB else 'B'].numpy()[0].transpose((1,2,0))\n",
    "    imgB = data['B' if AtoB else 'A'].numpy()[0].transpose((1,2,0))\n",
    "    path_PNM, path_SAR = data['A_paths' if AtoB else 'B_paths']\n",
    "    path_PNM, path_SAR = path_PNM[0], path_SAR[0]\n",
    "    city_name = os.path.basename(path_PNM).split(\"_\")[2].replace(\"-\", \" \")\n",
    "    info = info_df.iloc[i]\n",
    "    \n",
    "    if info['build pct 286'] < 0.01:\n",
    "        continue\n",
    "        \n",
    "    # compute patch areas\n",
    "    imgA = (imgA - imgA.min())/(imgA.max() - imgA.min()) * 255\n",
    "    imgB = (imgB - imgB.min())/(imgB.max() - imgB.min()) * 255\n",
    "    areas, mask = compute_patch_areas(imgB)\n",
    "    _, regions = get_regions(imgB, patches=range(10))\n",
    "    patch_sizes = np.array([len(r[0]) for r in regions])\n",
    "    f_polycentr = sum(patch_sizes[2:]) / float(sum(patch_sizes[:2])) \n",
    "    patch_stats.append([city_name, info['region'], f_polycentr])\n",
    "\n",
    "    # sample rays from city center: population\n",
    "    theta_pop, rays_pop = extract_rays(imgA[:,:,0], imgA.shape[0]/2, imgA.shape[1]/2, step=10, n_samples=100)    \n",
    "    rays_pop_mu = np.abs(rays_pop).mean(0); pop_scale = (rays_pop_mu.max() + 1e-5); \n",
    "    rays_pop_mu = rays_pop_mu / pop_scale\n",
    "    rays_pop_se = np.abs(rays_pop).std(0); rays_pop_se = rays_pop_se / np.sqrt(pop_scale)\n",
    "    dist_pop_mu.append(rays_pop_mu); dist_pop_se.append(rays_pop_se); \n",
    "\n",
    "    # sample rays from city center: luminosity\n",
    "    theta_lum, rays_lum = extract_rays(imgA[:,:,1], imgA.shape[0]/2, imgA.shape[1]/2, step=10, n_samples=100)\n",
    "    rays_lum_mu = np.abs(rays_lum).mean(0); lum_scale = (rays_lum_mu.max() + 1e-5)\n",
    "    rays_lum_mu = rays_lum_mu / lum_scale\n",
    "    rays_lum_se = np.abs(rays_lum).std(0); rays_lum_se = rays_lum_se / np.sqrt(lum_scale)    \n",
    "    dist_lum_mu.append(rays_lum_mu); dist_lum_se.append(rays_lum_se); \n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    clear_output(wait=True)\n",
    "    print \"%d / %d: %2.2f sec\"%(i, len(info_df), stop-start)\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "patch_stats = pd.DataFrame(patch_stats, \n",
    "                              columns=[\"city\", \"region\", \"pct polycenter\"])\n",
    "dist_pop_mu = np.vstack(dist_pop_mu)\n",
    "dist_lum_mu = np.vstack(dist_lum_mu)\n",
    "\n",
    "dist_pop_se = np.vstack(dist_pop_se)\n",
    "dist_lum_se = np.vstack(dist_lum_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save profile data\n",
    "\n",
    "with gzip.open(checkpoints_path + \"/profile_stats_%s.pickle.gz\"%opt.which_epoch, \"w\") as f:\n",
    "    pickle.dump([dist_pop_mu, dist_pop_se, \n",
    "                 dist_lum_mu, dist_lum_se, \n",
    "                 patch_stats],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1a258d0350>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD4CAYAAADmWv3KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b05aa3990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patch_stats['pct polycenter'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dist_pop_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate gradient magnitude relationship with distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# which_dataset = dataset_smpl\n",
    "# info_filename = data_loader_smpl.filename_csv\n",
    "\n",
    "which_dataset = dataset_valid\n",
    "info_filename = data_loader_valid.filename_csv\n",
    "\n",
    "info_df = pd.read_csv(info_filename)\n",
    "\n",
    "patches = [0] # main urban core\n",
    "# patches = [1,2,3] # top 2 connected settlements\n",
    "\n",
    "SHOW_PLOTS = False\n",
    "N_GRAD_SMPL= 30\n",
    "\n",
    "scenario_stats = []\n",
    "grad_dist_pop_mu = []\n",
    "grad_dist_lum_mu = []\n",
    "\n",
    "grad_dist_pop_se = []\n",
    "grad_dist_lum_se = []\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for i, data in enumerate(which_dataset):\n",
    "    if not SHOW_PLOTS:\n",
    "        clear_output(wait=True)\n",
    "        stop = timeit.default_timer()\n",
    "        print \"%d: %2.2f sec\"%(i, stop-start)\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "    AtoB = model.opt.which_direction == 'AtoB'\n",
    "    imgA = data['A' if AtoB else 'B']\n",
    "    imgB = data['B' if AtoB else 'A']\n",
    "    path = data['A_paths' if AtoB else 'B_paths'][0]\n",
    "    city_name = os.path.basename(path[0]).split(\"_\")[2].replace(\"-\", \" \")\n",
    "    info = info_df.iloc[i]\n",
    "    \n",
    "    if info['build pct 286'] < 0.01:\n",
    "        continue\n",
    "    \n",
    "    # get patches for which to compute average gradient\n",
    "    imgB_np = imgB.numpy()[0].transpose([1,2,0])\n",
    "    imgB_np = (imgB_np+1)/(imgB_np.max()+1.0) * 255\n",
    "    areas, mask = compute_patch_areas(imgB_np)\n",
    "    _, regions = get_regions(imgB_np, patches=patches)\n",
    "    \n",
    "    region_x = []\n",
    "    region_y = []\n",
    "    for r in regions:\n",
    "        region_x.append(r[0])\n",
    "        region_y.append(r[1])\n",
    "    region_x = np.hstack(region_x)\n",
    "    region_y = np.hstack(region_y)\n",
    "    idx      = np.random.choice(len(region_x), replace=False, \n",
    "                                size=min([len(region_x),N_GRAD_SMPL]))\n",
    "    region   = (region_x[idx], region_y[idx])\n",
    "\n",
    "    # run model, compute prediction and gradient\n",
    "    model.set_input(data)\n",
    "    img_path = model.get_image_paths()\n",
    "    model.test()\n",
    "    labels = model.get_current_labels()\n",
    "    mygrad = model.compute_gradient(region=region)\n",
    "    mygrad_np = mygrad.numpy()[0].transpose([1,2,0])\n",
    "    \n",
    "    if np.isnan(mygrad_np).sum() > 0:\n",
    "        break\n",
    "    \n",
    "    # format images for display\n",
    "    visuals = model.get_current_visuals()\n",
    "    imgA_np = copy.copy(visuals['real_A'])\n",
    "    imgB_np = copy.copy(visuals['real_B'])\n",
    "    imgB_np_fake = copy.copy(visuals['fake_B'])\n",
    "    \n",
    "    grad_pop = mygrad_np[:,:,0]/ (np.abs(mygrad_np[:,:,0]).min() + 1e-5)\n",
    "    grad_lum = mygrad_np[:,:,1]/ (np.abs(mygrad_np[:,:,1]).min() + 1e-5)\n",
    "\n",
    "    # apply mask to regions of interest\n",
    "    mymask = np.zeros(mask.shape)\n",
    "    for i,p in enumerate(patches):\n",
    "        a = areas[p]\n",
    "        mymask[mask==a[0]] = i+1\n",
    "    imgB_np[mymask>0] = 128\n",
    "    \n",
    "    # compute gradient within/without regions of interest\n",
    "    norm_grad_pop_within  = np.abs(grad_pop[mymask[:,:,0]>0]).mean()\n",
    "    norm_grad_pop_without = np.abs(grad_pop[mymask[:,:,0]==0]).mean()\n",
    "    norm_grad_lum_within  = np.abs(grad_lum[mymask[:,:,0]>0]).mean()\n",
    "    norm_grad_lum_without = np.abs(grad_lum[mymask[:,:,0]==0]).mean()\n",
    "    scenario_stats.append((city_name, info['region'], \n",
    "                           norm_grad_pop_without/(norm_grad_pop_within+norm_grad_pop_without + 1e-5), \n",
    "                           norm_grad_lum_without/(norm_grad_lum_within+norm_grad_lum_without + 1e-5)))\n",
    "    \n",
    "    # sample gradient magnitude along random rays to study dependence with distance\n",
    "    x0, y0 = regions[0][0].mean(), regions[0][1].mean()\n",
    "    theta_pop, rays_pop = extract_rays(grad_pop, x0, y0, step=10, n_samples=100)    \n",
    "    rays_pop_mu = np.abs(rays_pop).mean(0); pop_scale = (rays_pop_mu.max() + 1e-5); \n",
    "    rays_pop_mu = rays_pop_mu / pop_scale\n",
    "    rays_pop_se = np.abs(rays_pop).std(0); rays_pop_se = rays_pop_se / np.sqrt(pop_scale)\n",
    "    grad_dist_pop_mu.append(rays_pop_mu); grad_dist_pop_se.append(rays_pop_se); \n",
    "    \n",
    "    theta_lum, rays_lum = extract_rays(grad_lum, x0, y0, step=10, n_samples=100)\n",
    "    rays_lum_mu = np.abs(rays_lum).mean(0); lum_scale = (rays_lum_mu.max() + 1e-5)\n",
    "    rays_lum_mu = rays_lum_mu / lum_scale\n",
    "    rays_lum_se = np.abs(rays_lum).std(0); rays_lum_se = rays_lum_se / np.sqrt(lum_scale)    \n",
    "    grad_dist_lum_mu.append(rays_lum_mu); grad_dist_lum_se.append(rays_lum_se); \n",
    "    \n",
    "    if SHOW_PLOTS:\n",
    "        fig, ax = plt.subplots(1,5, figsize=(16,4))\n",
    "\n",
    "        ax[0].imshow(imgA_np)\n",
    "        ax[1].imshow(imgB_np)\n",
    "        ax[2].imshow(imgB_np_fake)\n",
    "        ax[3].imshow(grad_pop)\n",
    "        ax[4].imshow(grad_lum)\n",
    "\n",
    "        titles = [\"$x_A$ (Pop.+ Lum.)\", \"True map $x_B$\", \"Pred. map $\\hat x_B$\", \n",
    "                  \"$\\partial x_B/\\partial x_{pop}$\", \"$\\partial x_B/\\partial x_{lum}$\"]\n",
    "        for i in range(5):\n",
    "            ax[i].set_title(titles[i],  fontsize=16)\n",
    "            ax[i].axis(\"off\")\n",
    "\n",
    "        fig.suptitle(city_name, fontsize=18)\n",
    "        plt.subplots_adjust(top=1.0)\n",
    "        plt.show()\n",
    "\n",
    "scenario_stats = pd.DataFrame(scenario_stats, \n",
    "                              columns=[\"city\", \"region\", \"pct grad pop without\", \"pct grad lum without\"])\n",
    "grad_dist_pop_mu = np.vstack(grad_dist_pop_mu)\n",
    "grad_dist_lum_mu = np.vstack(grad_dist_lum_mu)\n",
    "\n",
    "grad_dist_pop_se = np.vstack(grad_dist_pop_se)\n",
    "grad_dist_lum_se = np.vstack(grad_dist_lum_se)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save gradient data\n",
    "\n",
    "with gzip.open(checkpoints_path + \"/gradient_stats_%s.pickle.gz\"%opt.which_epoch, \"w\") as f:\n",
    "    pickle.dump([grad_dist_pop_mu, grad_dist_pop_se, \n",
    "                 grad_dist_lum_mu, grad_dist_lum_se, \n",
    "                 scenario_stats],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-computed gradient data\n",
    "\n",
    "with gzip.open(checkpoints_path + \"/gradient_stats_%s.pickle.gz\"%opt.which_epoch, \"w\") as f:\n",
    "    (grad_dist_pop_mu, grad_dist_pop_se, grad_dist_lum_mu, grad_dist_lum_se, scenario_stats) = \\\n",
    "        pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of gradient magnitude within/without selected patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scenario_stats.dropna(inplace=True)\n",
    "scenario_stats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scenario_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "g = sns.FacetGrid(scenario_stats.rename(columns={\"pct grad pop without\":\"$|\\partial x_B/\\partial x_{pop}|$ [%]\"}),\n",
    "                  col=\"region\", margin_titles=True, sharey=True)\n",
    "g.map(sns.distplot, \"$|\\partial x_B/\\partial x_{pop}|$ [%]\")\n",
    "plt.subplots_adjust(top=0.7)\n",
    "g.fig.suptitle(\"Spatial gradient 'spillover': population density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "g = sns.FacetGrid(scenario_stats.rename(columns={\"pct grad lum without\":\"$|\\partial x_B/\\partial x_{lum}|$ [%]\"}),\n",
    "                  col=\"region\", margin_titles=True, sharey=True)\n",
    "g.map(sns.distplot, \"$|\\partial x_B/\\partial x_{lum}|$ [%]\")\n",
    "plt.subplots_adjust(top=0.7)\n",
    "g.fig.suptitle(\"Spatial gradient 'spillover': relative luminance (energy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "g = sns.FacetGrid(scenario_stats[scenario_stats['region']!='Oceania']\\\n",
    "                      .rename(columns={\"pct grad lum without\":\"$|\\partial x_B/\\partial x_{lum}|$ [%]\"}),\n",
    "                  col=\"region\", col_wrap=2, margin_titles=True, sharey=True)\n",
    "g.map(sns.distplot, \"$|\\partial x_B/\\partial x_{lum}|$ [%]\")\n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle(\"Spatial gradient 'spillover': \\n relative luminance (energy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study gradient dependence with distance from patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_ok = ~np.isnan(grad_dist_pop_mu.sum(1))\n",
    "grad_dist_pop_df = pd.concat([scenario_stats[['city', 'region']], \n",
    "                              pd.DataFrame(grad_dist_pop_mu[idx_ok,:])], 1).dropna()\n",
    "\n",
    "idx_ok = ~np.isnan(grad_dist_lum_mu.sum(1))\n",
    "grad_dist_lum_df = pd.concat([scenario_stats[['city', 'region']], \n",
    "                              pd.DataFrame(grad_dist_lum_mu[idx_ok,:])], 1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paris_pop_dist = grad_dist_pop_df[grad_dist_pop_df['city'].str.contains(\"paris, fr \")]\n",
    "paris_lum_dist = grad_dist_lum_df[grad_dist_lum_df['city'].str.contains(\"paris, fr \")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(np.arange(paris_lum_dist.shape[1]-2)*(10*200.0/256), \n",
    "         np.log10(paris_pop_dist.drop([\"city\", \"region\"],1).values.T), \n",
    "         label=\"pop. density\", lw=3)\n",
    "plt.plot(np.arange(paris_lum_dist.shape[1]-2)*(10*200.0/256), \n",
    "         np.log10(paris_lum_dist.drop([\"city\", \"region\"],1).values.T), \n",
    "         label=\"luminosity\", lw=3)\n",
    "plt.legend()\n",
    "plt.ylabel(\"log $|\\partial x_B/\\partial x_A|(d)$ (norm.)\")\n",
    "plt.xlabel(\"Distance $d$ [km]\")\n",
    "plt.title(\"Paris: spatial gradient\\n magnitude vs distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_dist_pop_df_mlt = pd.melt(grad_dist_pop_df.iloc[:,:20], \n",
    "                               id_vars=[\"region\", \"city\"])\n",
    "grad_dist_pop_df_mlt['log value'] = grad_dist_pop_df_mlt['value'].apply(np.log10)\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "g = sns.boxplot(data=grad_dist_pop_df_mlt, hue=\"region\",\n",
    "                y=\"log value\", x=\"variable\", showfliers=False)\n",
    "_ = g.set_xticklabels(((10*200./256)*np.arange(20)).astype(int), \n",
    "                      rotation=90)\n",
    "g.set_xlabel(\"distance from patch $d$ [km]\")\n",
    "g.set_ylabel(\"log $|\\partial x_B/\\partial x_{pop}|$ (normalized)\")\n",
    "g.set_title(\"Distance dependence of spatial gradient: pop. density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "g = sns.FacetGrid(grad_dist_pop_df_mlt, col=\"region\", \n",
    "                  margin_titles=True, sharey=True)\n",
    "g.map(sns.boxplot, \"variable\", \"log value\", showfliers=False)\\\n",
    "    .despine(left=True)\n",
    "_ = g.set_xticklabels(((10*200./256)*np.arange(26)).astype(int), \n",
    "                      rotation=90)\n",
    "g.set_xlabels(\"$d$ [km]\")\n",
    "g.set_ylabels(\"$log |\\partial x_B/\\partial x_{pop}|$ (norm.)\")\n",
    "g.fig.suptitle(\"Distance dependence of spatial gradient: pop. density\")\n",
    "plt.subplots_adjust(top=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_dist_lum_df_mlt = pd.melt(grad_dist_lum_df.iloc[:,:20], \n",
    "                               id_vars=[\"region\", \"city\"])\n",
    "grad_dist_lum_df_mlt['log value'] = grad_dist_lum_df_mlt['value'].apply(np.log10)\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "g = sns.boxplot(data=grad_dist_lum_df_mlt, hue=\"region\",\n",
    "                y=\"log value\", x=\"variable\", showfliers=False)\n",
    "_ = g.set_xticklabels(((10*200./256)*np.arange(20)).astype(int), \n",
    "                      rotation=90)\n",
    "g.set_xlabel(\"distance from patch $d$ [km]\")\n",
    "g.set_ylabel(\"$|\\partial x_B/\\partial x_{lum}|$ (normalized)\")\n",
    "g.set_title(\"Distance dependence of spatial gradient: luminosity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "g = sns.FacetGrid(grad_dist_lum_df_mlt, col=\"region\", \n",
    "                  margin_titles=True, sharey=True)\n",
    "g.map(sns.boxplot, \"variable\", \"log value\", showfliers=False)\\\n",
    "    .despine(left=True)\n",
    "_ = g.set_xticklabels(((10*200./256)*np.arange(26)).astype(int), \n",
    "                      rotation=90)\n",
    "g.set_xlabels(\"$d$ [km]\")\n",
    "g.set_ylabels(\"$|\\partial x_B/\\partial x_{lum}|$ (norm.)\")\n",
    "g.fig.suptitle(\"Distance dependence of spatial gradient: luminosity (energy)\")\n",
    "plt.subplots_adjust(top=0.7)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
