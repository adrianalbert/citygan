{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import sys, os, time\n",
    "import glob\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "# these magics ensure that external modules that are modified are also automatically reloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# widgets and interaction\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/nbserver/BEGAN-pytorch/\")\n",
    "import models, utils\n",
    "from datafolder import basic_preprocess, default_loader, flip_ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load experiment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_dir = \"/home/workspace/citygan/spatial-maps_17-11-15_22:52:17/\"\n",
    "\n",
    "num_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(experiment_dir + \"/params.json\") as f:    \n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load last checkpointed generator model\n",
    "\n",
    "files_cptG = glob.glob(experiment_dir + \"G*.pth\")\n",
    "files_cptG = {int(os.path.basename(f).split(\".\")[0].split(\"_\")[-1]):f \\\n",
    "              for f in files_cptG}\n",
    "netG_ckpt = files_cptG[4000] #np.max(files_cptG.keys())]\n",
    "print netG_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def load_weights_no_dataparallel(filename, **kwargs):\n",
    "    # original saved file with DataParallel\n",
    "    state_dict = torch.load(filename, **kwargs)\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if num_gpu == 0:\n",
    "    map_location = lambda storage, loc: storage\n",
    "else: \n",
    "    map_location = None\n",
    "\n",
    "n_channel = 1\n",
    "z_num = config['z_num']\n",
    "height = width = config['input_scale_size']\n",
    "repeat_num = int(np.log2(height)) - 2\n",
    "    \n",
    "netG = models.GeneratorCNN(z_num, [config['conv_hidden_num'], 8, 8], \n",
    "                          n_channel, repeat_num, config['conv_hidden_num'], \n",
    "                          num_gpu)\n",
    "weights = load_weights_no_dataparallel(netG_ckpt, \n",
    "                                       map_location=map_location)\n",
    "netG.load_state_dict(weights)\n",
    "\n",
    "if torch.cuda.is_available() and num_gpu>0:\n",
    "    netG.cuda()\n",
    "    \n",
    "netG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate synthetic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_samples(netG, Z, cond=None, cuda=False):\n",
    "    N, nz = Z.shape\n",
    "    Zv = torch.FloatTensor(Z.reshape(N, nz))\n",
    "    if cuda:\n",
    "        Zv = Zv.cuda()\n",
    "    if cond is not None:\n",
    "        ncond = cond.shape[1]\n",
    "        cond = torch.from_numpy(cond).float()\n",
    "        if cuda:\n",
    "            cond = cond.cuda()\n",
    "        cond = Variable(cond)\n",
    "        cond.data.resize_(N, ncond, 1, 1)\n",
    "    Zv = Variable(Zv)\n",
    "    fake = netG(Zv, cond=cond) if cond is not None else netG(Zv)\n",
    "    return fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "Z = np.random.randn(N, z_num)\n",
    "\n",
    "x_fake = generate_samples(netG, Z, cuda=True)\n",
    "\n",
    "utils.save_image_channels(x_fake.data, ncol=x_fake.size()[0], \n",
    "                          channel_names=config['src_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = x_fake.data.cpu().numpy().squeeze()\n",
    "\n",
    "tmp[np.abs(tmp)<0.003] = 0\n",
    "\n",
    "for i in range(10):\n",
    "    plt.imshow(tmp[i], cmap=cm.gray_r)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore latent space \n",
    " - a way to assess representation quality introduced in the DCGAN paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate interpolation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_interpolation_map(Z,M):\n",
    "    N, nz = Z.shape\n",
    "    Z_map = np.zeros((N-1,M+1,nz))\n",
    "    for n in range(N-1):\n",
    "        dZ = (Z[n+1,:] - Z[n,:]) / float(M)\n",
    "        for m in range(M+1):\n",
    "            Z_map[n,m,:] = Z[n,:] + m*dZ\n",
    "    Z_map = Z_map.reshape(((N-1)*(M+1), nz))\n",
    "    return Z_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 10\n",
    "\n",
    "Z_map = generate_interpolation_map(Z[[0,1]], M)\n",
    "x_fake_interp = generate_samples(netG, Z_map, cuda=True)\n",
    "\n",
    "utils.save_image_channels(x_fake_interp.data, ncol=x_fake_interp.size()[0], \n",
    "                          channel_names=config['src_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morph between real samples\n",
    "Recover latent vector of a real image by optimizing a L2 norm of the reconstruction loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize_channels = ([0, 160.91, 1.295, 0, 0],\n",
    "                      [1, 1010.413, 15.326, 1, 1])\n",
    "normalize = normalize_channels\n",
    "take_log = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/home/data/world-cities/spatial-maps/samples/\"\n",
    "files = glob.glob(data_path + \"/*.tif\")\n",
    "files = {\", \".join(os.path.basename(f).split(\"_\")[:2][::-1]).replace(\"-\",\" \"):f \\\n",
    "          for f in files}\n",
    "files.keys()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycities = [\"paris, france\", \"rio de janeiro, brazil\",\n",
    "            \"san francisco, united states\", \"dhaka, bangladesh\"]\n",
    "\n",
    "city_start = \"paris, france\"\n",
    "city_target= \"dhaka, bangladesh\"\n",
    "\n",
    "img_start  = imread(files[city_start])\n",
    "img_target = imread(files[city_target])\n",
    "\n",
    "img_start  = basic_preprocess(img_start, width, log=take_log, normalize=normalize)\n",
    "img_target = basic_preprocess(img_target, width, log=take_log, normalize=normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from latent_space import reverse_z\n",
    "\n",
    "print \"learning latent representation for start image\"\n",
    "z_start, loss_start = reverse_z(netG.cuda(), img_start, nz=z_num, cuda=True, \n",
    "                    clip='stochastic', lr=0.0012, niter=5000)\n",
    "\n",
    "print \"learning latent representation for target image\"\n",
    "z_target, loss_target = reverse_z(netG.cuda(), img_target, nz=z_num, cuda=True, \n",
    "                    clip='stochastic', lr=0.0012, niter=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "t = np.arange(0.01, 10.0, 0.01)\n",
    "ax1.plot(loss_start, lw=3, color='k')\n",
    "ax1.set_xlabel('# iterations')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('loss (start city)', color='k')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(loss_target, lw=3)\n",
    "ax2.set_ylabel('loss (target city)', color=\"b\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.title(\"Recovering latent vectors $z_{start}$, $z_{target}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 10\n",
    "Z = np.array([z_start.cpu().data.numpy(), \n",
    "              z_target.cpu().data.numpy()]).squeeze()\n",
    "Z_map = generate_interpolation_map(Z, M)\n",
    "x_fake_interp = generate_samples(netG.cpu(), Z_map)\n",
    "\n",
    "utils.save_image_channels(x_fake_interp.data, ncol=x_fake_interp.size()[0], \n",
    "                          channel_names=config['src_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features for test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get checkpoints\n",
    "\n",
    "files_cptD = glob.glob(experiment_dir + \"/netD*.pth\")\n",
    "files_cptD = {int(os.path.basename(f).split(\".\")[0].split(\"_\")[-1]):f \\\n",
    "              for f in files_cptD}\n",
    "\n",
    "last_checkpoint = files_cptD[np.argmax(files_cptD.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../models/\")\n",
    "import models.dcgan_orig as do\n",
    "\n",
    "ngpu = 2\n",
    "netD = do._netD(ngpu, nc, ndf)\n",
    "\n",
    "netD.load_state_dict(torch.load(last_checkpoint))\n",
    "if torch.cuda.is_available():\n",
    "    netD.cuda()\n",
    "\n",
    "feature_extractor = nn.Sequential(*list(list(netD.children())[0].children())[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up test data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"./../models/pytorch_utils\")\n",
    "from loader_dataframe import ImageDataFrame, grayscale_loader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "test_df = pd.read_csv(dataroot + \"/test.csv\")\n",
    "\n",
    "dataset = ImageDataFrame(df=test_df, classCol='city',\n",
    "                         loader=grayscale_loader,\n",
    "                         transform=transforms.Compose([\n",
    "                               transforms.Scale(imageSize),\n",
    "                               transforms.CenterCrop(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "assert dataset\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize,\n",
    "                                     shuffle=False, num_workers=int(workers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def extract_features(feature_extractor, X):\n",
    "    '''\n",
    "    X is a list of data batches or a generator (dataloader).\n",
    "    '''\n",
    "    input = torch.FloatTensor(batchSize, nc, imageSize, imageSize)\n",
    "    if torch.cuda.is_available():\n",
    "        input = input.cuda()\n",
    "    input = Variable(input)\n",
    "\n",
    "    labels = []\n",
    "    features = []\n",
    "    for i, data in enumerate(X):\n",
    "        feature_extractor.zero_grad()\n",
    "        real_cpu, lab_batch = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "        input.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "        feat_batch = feature_extractor(input)\n",
    "        feat_batch = feat_batch.data.cpu().numpy().reshape((batch_size,-1))\n",
    "\n",
    "        features.append(feat_batch)\n",
    "        if lab_batch is not None:\n",
    "            lab_batch = lab_batch.numpy()\n",
    "        labels.append(lab_batch)\n",
    "\n",
    "    features = np.vstack(features)\n",
    "    labels = np.hstack(labels)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, labels = extract_features(feature_extractor, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "# import gzip\n",
    "\n",
    "# with gzip.open(\"test_set_features.pickle.gz\", \"r\") as f:\n",
    "#     features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open(\"test_set_features.pickle.gz\", \"w\") as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate lots of synthetic samples\n",
    "- pass them through the Discriminator to obtain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100000\n",
    "Z = np.random.randn(N, nz)\n",
    "\n",
    "fake = generate_samples(Z, cuda=False)\n",
    "# img_fake = fake.data.numpy().squeeze()\n",
    "\n",
    "n_batches = int(np.ceil(float(N)/batchSize))\n",
    "fake_batches = [(fake.data[(i*batchSize):min([N,((i+1)*batchSize)])], None) \\\n",
    "                for i in range(n_batches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_fake = fake.data.numpy().squeeze()\n",
    "img_fake = 1 - np.abs(img_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_fake, labels_fake = extract_features(feature_extractor, fake_batches)\n",
    "\n",
    "features_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_fake = pd.Series([img.mean() for img in img_fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_fake[p_fake<0.45].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "large_fake_img = img_fake[(p_fake >= 0.14) & (p_fake <= 0.15)]\n",
    "print len(large_fake_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in large_fake_img[:20]:\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project to low-d embedding space via t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "N_PCA = 50\n",
    "\n",
    "pca = decomposition.PCA(n_components=N_PCA)\n",
    "\n",
    "feats_pca = pca.fit_transform(features)[:,:N_PCA]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"PCA Components Variance Explained\")\n",
    "plt.xlabel(\"# components\")\n",
    "plt.ylabel(\"% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/DmitryUlyanov/Multicore-TSNE\n",
    "\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "tsne = TSNE(n_jobs=32, perplexity=30, n_components=20)\n",
    "feats_tsne = tsne.fit_transform(features.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats_tsne.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar image search: real cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "from sklearn.neighbors import KDTree, BallTree\n",
    "\n",
    "myfeatures = features # feats_tsne\n",
    "\n",
    "tree = BallTree(myfeatures, leaf_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mycities = [\"barcelona, es\", \"athens, gr\", \"berlin, de\", \n",
    "#             \"hyderabad, in\",\n",
    "#             \"boston, us\", \"san francisco, us\"]\n",
    "mycities = [\"san francisco, us\", \"paris, fr\", \"delhi, in\"]\n",
    "sel_df = test_df[test_df['city'].apply(lambda s: sum([x in s for x in mycities])>0)]\n",
    "\n",
    "feats_sel = myfeatures[sel_df.index,:]\n",
    "\n",
    "n_examples = 4\n",
    "_, neighbors = tree.query(feats_sel, k=n_examples, dualtree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "\n",
    "# create canvass\n",
    "canvass = []\n",
    "for city0,f0,idx in zip(sel_df['city'], sel_df['filename'], neighbors):\n",
    "    img0 = imread(f0)\n",
    "    # imsave(experiment_dir + \"./example_%s\"%city0, img0)\n",
    "    row = [np.pad(resize(img0, (100,100)), (2,2), \"constant\")]\n",
    "    for _,r in test_df.ix[idx[1:]].iterrows():\n",
    "        img = imread(r['filename'])\n",
    "        row.append(np.pad(resize(img, (100,100)), (2,2), \"constant\"))\n",
    "    canvass.append(np.hstack(row))\n",
    "canvass = np.vstack(canvass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=2, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(canvass)\n",
    "plt.yticks([])\n",
    "plt.yticks([52 + i * 104 for i in range(len(sel_df))], [x.split(\"(\")[0].replace(\", \", \"\\n\") for x in sel_df['city']])\n",
    "plt.xticks([52 + i * 104 for i in range(n_examples)], [\"query\"] + [\"nghbr.%d\"%i for i in range(n_examples)])\n",
    "plt.title(\"Similar Cities\")\n",
    "\n",
    "for i,(city0,f0,idx) in enumerate(zip(sel_df['city'], sel_df['filename'], neighbors)):\n",
    "    for j,(_,r) in enumerate(test_df.ix[idx].iterrows()):\n",
    "        city1 = r['city']\n",
    "        plt.annotate(city1.split(\"(\")[0].replace(\", \", \"\\n\"), \n",
    "                     xy=(5 + j * 104, 35 + i * 104), \n",
    "                     xytext=(5 + j * 104, 35 + i * 104),\n",
    "                     fontsize=16, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search similar images: simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_fake = KDTree(features_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycities = [\"san francisco, us\", \"paris, fr\", \"delhi, in\", \n",
    "            \"lagos, ng\", \"bucharest, ro\", \"rome, it\"]\n",
    "sel_df = test_df[test_df['city'].apply(lambda s: sum([x in s for x in mycities])>0)]\n",
    "feats_sel = features[sel_df.index,:]\n",
    "\n",
    "# feats_sel = features_fake[idx_14,:]\n",
    "# feats_sel = feats_sel[np.random.choice(range(len(feats_sel)), 5, replace=False),:]\n",
    "\n",
    "n_examples = 5\n",
    "_, neighbors = tree_fake.query(feats_sel, k=n_examples, breadth_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create canvass\n",
    "canvass = []\n",
    "for city0,f0,idx in zip(sel_df['city'], sel_df['filename'], neighbors):\n",
    "    img0 = imread(f0)\n",
    "    row = []\n",
    "    row = [np.pad(resize(img0, (100,100)), (2,2), \"constant\")]\n",
    "    for img in img_fake[idx,:]:\n",
    "        row.append(np.pad(resize(img, (100,100)), (2,2), \"constant\"))\n",
    "    canvass.append(np.hstack(row))\n",
    "canvass = np.vstack(canvass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=2, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(canvass)\n",
    "plt.yticks([])\n",
    "plt.yticks([52 + i * 104 for i in range(len(sel_df))], \\\n",
    "             [x.split(\"(\")[0].replace(\", \", \"\\n\") for x in sel_df['city']])\n",
    "plt.xticks([52 + i * 104 for i in range(n_examples+1)], \\\n",
    "           [\"query\"] + [\"nghbr.%d\"%i for i in range(n_examples)])\n",
    "# plt.title(\"Simulations similar to real cities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "feats_reduced = feats_tsne #feats_pca\n",
    "\n",
    "loss_vec = []\n",
    "k_vec = np.linspace(5, 100, 40)\n",
    "for k in k_vec:\n",
    "    print int(k),\n",
    "    kmeans = KMeans(n_clusters=int(k), random_state=0).fit(feats_reduced)\n",
    "    loss = -kmeans.score(feats_reduced)\n",
    "    loss_vec.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(k_vec, np.array(loss_vec)/1e6)\n",
    "plt.title(\"K-Means loss vs # clusters\")\n",
    "plt.xlabel(\"# Clusters\")\n",
    "plt.ylabel(\"loss (/1e6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K_opt = 25\n",
    "kmeans = KMeans(n_clusters=K_opt, random_state=0).fit(feats_tsne)\n",
    "\n",
    "C = kmeans.predict(feats_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(pd.Series(C).value_counts() / float(len(C))).plot(kind=\"barh\", figsize=(4,4))\n",
    "plt.title(\"Cluster Membership Distribution\")\n",
    "plt.xlabel(\"pct membership\")\n",
    "plt.ylabel(\"cluster ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_examples(image_paths, labels, classes=None, \\\n",
    "                  nExamples=10, thumbSize = (64,64), pad_pix=2, title=\"example\"):\n",
    "    # build example canvass \n",
    "    from skimage.transform import resize\n",
    "    from skimage.io import imread\n",
    "    \n",
    "    clustLabels = np.unique(labels)\n",
    "    nClusters = clustLabels.size\n",
    "    canvas = np.ones(((thumbSize[0]+pad_pix)*nClusters, nExamples*(thumbSize[1]+pad_pix)))\n",
    "    for i,c in enumerate(clustLabels):\n",
    "        cur_class_samples = np.where(labels==c)[0]\n",
    "        idx = np.random.choice(cur_class_samples, replace=False, size=min([nExamples, len(cur_class_samples)]))\n",
    "        for j in range(len(idx)):\n",
    "            img = imread(image_paths[idx[j]])\n",
    "            img = img / float(img.max())\n",
    "            img[abs(img-0.5)<0.01] = 0 # hack to remove no-data patches\n",
    "            # img = 1-img\n",
    "            img = resize(img, thumbSize)\n",
    "            canvas[i*(thumbSize[0]+pad_pix):(i*pad_pix + (i+1)*thumbSize[0]), \n",
    "                   j*(thumbSize[1]+pad_pix):(j*pad_pix + (j+1)*thumbSize[1])] = img\n",
    "    \n",
    "    # plot examples of each class\n",
    "    fig,ax = plt.subplots(1, figsize=(12,10))\n",
    "    plt.tight_layout()\n",
    "    print canvas.shape\n",
    "    ax.imshow(canvas.swapaxes(0,1))#, aspect='auto')\n",
    "    ax.set_title(title, fontsize=18)\n",
    "    ax.set_ylabel(\"-- examples --\", fontsize=16)\n",
    "    ax.set_xlabel(\"-- land classes --\", fontsize=16)\n",
    "    # Turn off tick labels\n",
    "    if classes is None: classes = clustLabels\n",
    "    ax.set_xticks([thumbSize[0]*(0.5 + x) for x in range(nClusters)])\n",
    "    ax.set_xticklabels(classes, fontsize=16, rotation=90)\n",
    "    ax.set_yticklabels([])\n",
    "    #plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_examples(test_df['filename'].values, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualize clusters\n",
    "- linear grid assignment visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard t-SNE point clouds labeled for the clusters identified by K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "def labeled_scatterplot(x, labels, palette=None, ax=None, add_text=True, alpha=None, figsize=None):\n",
    "    \n",
    "    classes = np.unique(labels)\n",
    "    class_dict = {c:i for i,c in enumerate(classes)}\n",
    "    n_clust = len(classes)    \n",
    "    colors = [palette[l] for l in labels]\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    if ax is None:\n",
    "        f = plt.figure(figsize=figsize)\n",
    "        ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=colors, alpha=alpha)\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    if add_text:\n",
    "        # We add the labels for each class.\n",
    "        txts = []\n",
    "        for k,c in enumerate(classes):\n",
    "            # Position of each label.\n",
    "            xtext, ytext = np.median(x[labels == c, :], axis=0)\n",
    "            txt = ax.text(xtext, ytext, str(c), fontsize=24)\n",
    "            txt.set_path_effects([\n",
    "                PathEffects.Stroke(linewidth=5, foreground=palette[c]),\n",
    "                PathEffects.Normal()])\n",
    "            txts.append(txt)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_jobs=32, perplexity=30, n_components=2)\n",
    "feats_tsne_2d = tsne.fit_transform(features.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette(\"hls\", K_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "palette = dict(zip(range(K_opt), sns.color_palette(\"hls\", K_opt)))\n",
    "labeled_scatterplot(feats_tsne_2d, C, palette=palette, ax=None, \\\n",
    "                    add_text=True, figsize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stretch point cloud to uniform grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "size = 50\n",
    "\n",
    "grid = np.dstack(np.meshgrid(np.linspace(0, 1, size), np.linspace(0, 1, size))).reshape(-1, 2)\n",
    "\n",
    "idx = np.random.choice(range(len(feats_tsne_2d)), size*size, replace=False)\n",
    "cost_matrix = cdist(grid, feats_tsne_2d[idx,:], \"sqeuclidean\").astype(np.float32)\n",
    "cost_matrix = cost_matrix * (100000 / cost_matrix.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lapjv\n",
    "\n",
    "_, row_asses, col_asses = lapjv.lapjv(cost_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_jv = grid[row_asses]\n",
    "pp_cmap = matplotlib.colors.ListedColormap(palette.values())\n",
    "for start, end, t in zip(feats_tsne_2d[idx,:], grid_jv, C[idx]):\n",
    "    plt.arrow(start[0], start[1], end[0] - start[0], end[1] - start[1],\n",
    "          head_length=0.005, head_width=0.005, color=pp_cmap(t), alpha=0.5)\n",
    "#colorbar(my_colorbar.mappable, fraction=0.05, pad = 0.0125)\n",
    "plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regions_df = pd.read_csv(\"/home/adalbert/data/countries_regions.csv\")\n",
    "\n",
    "country2region = {r:c for r,c in zip(regions_df['alpha-2'].str.lower(), regions_df['region'])}\n",
    "\n",
    "test_df['region'] = test_df['country'].apply(lambda x: country2region[x] if x in country2region else np.nan)\n",
    "\n",
    "print \"Could not map to regions %d out of %d cities.\"%(test_df['region'].isnull().sum(), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = test_df[[\"filename\", 'region', 'decile', 'built pct', 'class']]\n",
    "df['cluster'] = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby(\"region\").apply(lambda x: x['cluster'].value_counts() / float(len(x))).unstack()\\\n",
    "    .plot(kind=\"bar\", stacked=True, figsize=(6,4), cmap=pp_cmap)\n",
    "plt.legend(bbox_to_anchor=(1.2, 1.15), fontsize=12)\n",
    "plt.title(\"World-wide distribution of city types\", fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats = df.groupby([\"cluster\"]).apply(lambda x: (x['built pct'].mean().round(3), \n",
    "                                         x['built pct'].std().round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.boxplot(column=\"built pct\", by=\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby(\"cluster\").apply(len).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = df['cluster']==0\n",
    "plot_examples(df[idx]['filename'].values, \n",
    "              df[idx]['region'].values, nExamples=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
